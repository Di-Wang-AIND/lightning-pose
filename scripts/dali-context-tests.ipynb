{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from nvidia.dali import pipeline_def\n",
    "import nvidia.dali.fn as fn\n",
    "from nvidia.dali.plugin.pytorch import LastBatchPolicy\n",
    "from nvidia.dali.plugin.pytorch import DALIGenericIterator\n",
    "import nvidia.dali.types as types\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Optional, Union, Literal, Tuple\n",
    "\n",
    "# see https://pytorch.org/vision/stable/models.html\n",
    "_IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "_IMAGENET_STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline_def\n",
    "def video_pipe(\n",
    "    filenames: Union[List[str], str],\n",
    "    resize_dims: Optional[List[int]] = None,\n",
    "    random_shuffle: bool = False,\n",
    "    seed: int = 123456,\n",
    "    sequence_length: int = 16,\n",
    "    pad_sequences: bool = True,\n",
    "    initial_fill: int = 16,\n",
    "    normalization_mean: List[float] = _IMAGENET_MEAN,\n",
    "    normalization_std: List[float] = _IMAGENET_STD,\n",
    "    device: str = \"gpu\",\n",
    "    name: str = \"reader\",\n",
    "    step: int = 1,\n",
    "    pad_last_batch: bool = False,\n",
    "    # arguments consumed by decorator:\n",
    "    # batch_size,\n",
    "    # num_threads,\n",
    "    # device_id\n",
    ") -> tuple:\n",
    "    \"\"\"Generic video reader pipeline that loads videos, resizes, augments, and normalizes.\n",
    "\n",
    "    Args:\n",
    "        filenames: list of absolute paths of video files to feed through\n",
    "            pipeline\n",
    "        resize_dims: [height, width] to resize raw frames\n",
    "        random_shuffle: True to grab random batches of frames from videos;\n",
    "            False to sequential read\n",
    "        seed: random seed when `random_shuffle` is True\n",
    "        sequence_length: number of frames to load per sequence\n",
    "        pad_sequences: allows creation of incomplete sequences if there is an\n",
    "            insufficient number of frames at the very end of the video\n",
    "        initial_fill: size of the buffer that is used for random shuffling\n",
    "        normalization_mean: mean values in (0, 1) to subtract from each channel\n",
    "        normalization_std: standard deviation values to subtract from each\n",
    "            channel\n",
    "        device: \"cpu\" | \"gpu\"\n",
    "        name: pipeline name, used to string together DataNode elements\n",
    "        step: number of frames to advance on each read\n",
    "        pad_last_batch\n",
    "\n",
    "    Returns:\n",
    "        pipeline object to be fed to DALIGenericIterator\n",
    "\n",
    "    \"\"\"\n",
    "    video = fn.readers.video(\n",
    "        device=device,\n",
    "        filenames=filenames,\n",
    "        random_shuffle=random_shuffle,\n",
    "        seed=seed,\n",
    "        sequence_length=sequence_length,\n",
    "        step=step,\n",
    "        pad_sequences=pad_sequences,\n",
    "        initial_fill=initial_fill,\n",
    "        normalized=False,\n",
    "        name=name,\n",
    "        dtype=types.DALIDataType.FLOAT,\n",
    "        pad_last_batch=pad_last_batch,  # Important for context loaders\n",
    "        file_list_include_preceding_frame=True,  # to get rid of dali warnings\n",
    "    )\n",
    "    if resize_dims:\n",
    "        video = fn.resize(video, size=resize_dims)\n",
    "    # video pixel range is [0, 255]; transform it to [0, 1].\n",
    "    # happens naturally in the torchvision transform to tensor.\n",
    "    video = video / 255.0\n",
    "    # permute dimensions and normalize to imagenet statistics\n",
    "    transform = fn.crop_mirror_normalize(\n",
    "        video,\n",
    "        output_layout=\"FCHW\",\n",
    "        mean=normalization_mean,\n",
    "        std=normalization_std,\n",
    "    )\n",
    "    return transform\n",
    "\n",
    "\n",
    "def count_frames(video_list: Union[List[str], str]) -> int:\n",
    "    \"\"\"Simple function to count the number of frames in a video or a list of videos.\"\"\"\n",
    "    if isinstance(video_list, str):\n",
    "        video_list = [video_list]\n",
    "    num_frames = 0\n",
    "    for video_file in video_list:\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        num_frames += int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        cap.release()\n",
    "\n",
    "    return num_frames\n",
    "\n",
    "\n",
    "class LitDaliWrapper(DALIGenericIterator):\n",
    "    \"\"\"wrapper around a DALI pipeline to get batches for ptl.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        num_iters: int = 1,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"Wrapper around DALIGenericIterator to get batches for pl.\n",
    "\n",
    "        Args:\n",
    "            num_iters: number of enumerations of dataloader (should be computed outside for now;\n",
    "                should be fixed by lightning/dali teams)\n",
    "\n",
    "        \"\"\"\n",
    "        self.num_iters = num_iters\n",
    "        self.batch_sampler = 1  # hack to get around DALI-ptl issue\n",
    "        # call parent\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_iters\n",
    "\n",
    "    def __next__(self):\n",
    "        return super().__next__()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "provide a path to some video file for testing -- here is one test video from this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file = \"../toy_datasets/toymouseRunningData/unlabeled_videos/test_vid.mp4\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Context loader\n",
    "Here are we define a pipeline that reads sequences of 5 frames at a time.\n",
    "In practice we have a model that takes 5 frames as input and outputs a pose for the 3rd frame, but we omit the model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# args for context loader\n",
    "pipe_args = {\n",
    "    \"filenames\": video_file,\n",
    "    \"resize_dims\": [64, 64],\n",
    "    \"sequence_length\": 5,\n",
    "    \"step\": 1,\n",
    "    \"batch_size\": 16,\n",
    "    \"num_threads\": 4,\n",
    "    \"device_id\": 0,\n",
    "    \"random_shuffle\": False,\n",
    "    \"device\": \"gpu\",\n",
    "    \"name\": \"reader\",\n",
    "    \"pad_sequences\": True,\n",
    "    \"pad_last_batch\": True,\n",
    "}\n",
    "\n",
    "pipe = video_pipe(**pipe_args)\n",
    "\n",
    "# set up parameters for pytorch iterator\n",
    "frame_count = count_frames(video_file)\n",
    "# taken from https://github.com/danbider/lightning-pose/blob/b66fe34719ec89631f74c6c911a5e1a013bc7e34/lightning_pose/data/dali.py#L237\n",
    "\n",
    "# this is for the base prediction, no context\n",
    "# num_iters = int(np.ceil(frame_count / pipe_args[\"sequence_length\"]))\n",
    "\n",
    "# context loader, different way of calculating num_iters\n",
    "num_iters = int(np.ceil(frame_count / (pipe_args[\"batch_size\"])))\n",
    "\n",
    "iterator_args = {\n",
    "    \"num_iters\": num_iters,\n",
    "    \"output_map\": [\"frames\"],\n",
    "    \"last_batch_policy\": LastBatchPolicy.FILL,\n",
    "    \"auto_reset\": False,\n",
    "    \"reader_name\": \"reader\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build iterator\n",
    "iterator = LitDaliWrapper(pipe, **iterator_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:30<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to iterate over 63 batches: 30.634568452835083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# iterate over data\n",
    "from time import time\n",
    "start_time = time()\n",
    "for batch in tqdm(iterator):\n",
    "    shape =batch[0][\"frames\"].shape # we don't print now to avoid cluttering the notebook\n",
    "end_time = time()\n",
    "print(f\"Time to iterate over {num_iters} batches: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = end_time - start_time\n",
    "time_per_iter = total_time / num_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4862629913148426"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_per_iter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base prediction\n",
    "Now define the args for base prediction. Just iterate over sequences until we reach the end of the video. In practice we have a standard resnet waiting for these frames and evaluated in the standard way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_file = \"/home/jovyan/datastores/mirror-mouse/videos_new/180607_004.mp4\"\n",
    "\n",
    "# frame_count = count_frames(video_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args for base predict loader\n",
    "pipe_args = {\n",
    "    \"filenames\": video_file,\n",
    "    \"resize_dims\": [64, 64],\n",
    "    \"sequence_length\": 64,\n",
    "    \"step\": 64,\n",
    "    \"batch_size\": 1,\n",
    "    \"num_threads\": 4, # was 4\n",
    "    \"device_id\": 0,\n",
    "    \"random_shuffle\": False,\n",
    "    \"device\": \"gpu\", \n",
    "    \"name\": \"reader\",\n",
    "    \"pad_sequences\": True,\n",
    "}\n",
    "\n",
    "num_iters = int(np.ceil(frame_count / pipe_args[\"sequence_length\"]))\n",
    "\n",
    "# https://github.com/danbider/lightning-pose/blob/0d9c26c42cbddbd16a8f01937d714d221474225d/lightning_pose/data/dali.py#L386\n",
    "iterator_args = {\n",
    "    \"num_iters\": num_iters,\n",
    "    \"output_map\": [\"frames\"],\n",
    "    \"last_batch_policy\": LastBatchPolicy.FILL,\n",
    "    \"last_batch_padded\": False,\n",
    "    \"auto_reset\": False,\n",
    "    \"reader_name\": \"reader\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = video_pipe(**pipe_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build iterator\n",
    "iterator = LitDaliWrapper(pipe, **iterator_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:23<00:00, 19.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to iterate over 469 batches: 23.523786544799805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# iterate over data\n",
    "from time import time\n",
    "start_time = time()\n",
    "for batch in tqdm(iterator):\n",
    "    shape =batch[0][\"frames\"].shape # we don't print now to avoid cluttering the notebook\n",
    "end_time = time()\n",
    "print(f\"Time to iterate over {num_iters} batches: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per iteration: 0.050110924726864424\n"
     ]
    }
   ],
   "source": [
    "total_time = end_time - start_time\n",
    "time_per_iter = total_time / num_iters\n",
    "print(f\"Time per iteration: {time_per_iter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 994 // num_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test saving to .npy file, batch by batch\n",
    "# batch_dims = (64, 2000, 16, 16)\n",
    "# num_batches_to_test = 7\n",
    "# test_filename = \"/home/jovyan/test.npy\"\n",
    "\n",
    "# # create fake data and save on the fly to a single .npy file\n",
    "# with open('test.npy', 'wb') as f:\n",
    "#     for i in range(num_batches_to_test):\n",
    "#         fake_data = np.ones(batch_dims, dtype=np.float16) * i\n",
    "#         np.save(f, fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the data back in\n",
    "# with open('test.npy', 'rb') as f:\n",
    "#     for i in range(num_batches_to_test):\n",
    "#         loaded_data = np.load(f)\n",
    "#         print(loaded_data.shape)\n",
    "#         print(np.unique(loaded_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 2000, 16, 16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loaded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2db7ebd2a6eaabbedad5619cf81ba50362feaaec41f65baf0d1ccad0b63e6ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
